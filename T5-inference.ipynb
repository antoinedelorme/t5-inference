{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "client = BigQueryClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECT_ID = 'for-antoine'\n",
    "DATASET_GCP_PROJECT_ID = GCP_PROJECT_ID # A copy of the data is saved in the user project\n",
    "DATASET_ID = 'MESH_CLASSIFICATION'\n",
    "TRAIN_TABLE_ID = 'jama_no_keywords'\n",
    "\n",
    "FEATURES = ['id','title','abstract']\n",
    "DTYPES=[tf.string] * len(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= f\"\"\"\n",
    "   SELECT *\n",
    "   FROM `for-antoine.MESH_CLASSIFICATION.jama_no_keywords`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "def read_session(TABLE_ID):\n",
    "    return client.read_session(\n",
    "        \"projects/\" + GCP_PROJECT_ID, DATASET_GCP_PROJECT_ID, TABLE_ID, DATASET_ID,\n",
    "        FEATURES , DTYPES, requested_streams=2\n",
    ")\n",
    "\n",
    "def extract_question(input_dict):\n",
    "    features = dict(input_dict)\n",
    "    id = features['id']\n",
    "    question = 'bioasq organism: ' + 'journal: jama title: ' + features[\"title\"] +  ' abstract: ' + tf.strings.lower(features[\"abstract\"])\n",
    "    return (id,question)\n",
    "\n",
    "\n",
    "raw_train_data = read_session(TRAIN_TABLE_ID).parallel_read_rows().map(extract_question).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_export(batch_size, model_name, model_size, model_version, model_number, prefix, candidates):\n",
    "    if batch_size != BATCH_SIZE:\n",
    "        print('ERROR: error in batch size')\n",
    "    return  {'batch_size': batch_size, 'model_name':model_name, 'model_size':model_size, 'model_version':model_version, 'model_number':model_number, 'prefix': prefix, 'candidates':candidates}\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "batch_size = 64\n",
    "model_name = 'BIOASQ-organism'\n",
    "model_size = 'base'\n",
    "model_version = 'general'\n",
    "model_number = '1592512738'\n",
    "prefix = 'bioasq organism: '\n",
    "candidates = ['Antibodies','Monoclonal','Receptors','Recombinant Proteins','Amino Acid Sequence','Cells','DNA', 'Viral', 'Genes','Antineoplastic Agents', 'Binding Sites', 'Antigens', 'Tumor Cells', 'Neoplasms', 'Neurons', 'Liver', 'Brain', 'Escherichia coli', 'Tumor', 'Muscle', 'Kidney', 'Chromosomes']\n",
    "\n",
    "model = model_export(batch_size, model_name, model_size, model_version, model_number, prefix, candidates)\n",
    "models.append(model)\n",
    "\n",
    "batch_size = 64\n",
    "model_name = 'BIOASQ-techniques'\n",
    "model_size = 'base'\n",
    "model_version = 'general'\n",
    "model_number = '1592657921'\n",
    "prefix = 'bioasq techniques: '\n",
    "candidates = ['Prognosis','Retrospective Studies','Treatment Outcome','Disease Models','Cultured','Cloning','Electrophoresis','Molecular Sequence Data','Polymerase Chain Reaction','Severity of Illness Index','Injections']\n",
    "model = model_export(batch_size, model_name, model_size, model_version, model_number, prefix, candidates)\n",
    "models.append(model)\n",
    "\n",
    "batch_size = 64\n",
    "model_name = 'BIOASQ-categories'\n",
    "model_size = 'base'\n",
    "model_version = 'general'\n",
    "model_number = '1592659655'\n",
    "prefix = 'bioasq general: '\n",
    "candidates = ['Animals', 'Male', 'Rabbits', 'Adult', 'Humans', 'Dogs', 'Cattle','Animal', 'Female', 'Mice', 'Human', 'Infant', 'Adolescent', 'Middle Aged', 'Young Adult', 'Age Factors', 'Aged', '80 and over', 'Rats', 'Child', 'Preschool', 'Sprague-Dawley','Swine','Newborn']\n",
    "\n",
    "\n",
    "model = model_export(batch_size, model_name, model_size, model_version, model_number, prefix, candidates)\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(pairs):\n",
    "    \"\"\"\n",
    "    Returns the final prediction of test data in a list\n",
    "            Parameters:\n",
    "            list of pair for the predict function \n",
    "\n",
    "            Returns:\n",
    "            list of final prediction  \n",
    "    \"\"\"\n",
    "    final_predictions=[]\n",
    "    predicted = predict_fn(pairs)\n",
    "    for vals in predicted:\n",
    "        vals = vals.decode('utf-8')\n",
    "        final_predictions.append(vals)\n",
    "    return final_predictions\n",
    "\n",
    "def generate_keywords(predictions, candidates):\n",
    "    results = []\n",
    "    for prediction in predictions:\n",
    "        result = ''\n",
    "        keywords = []\n",
    "        prediction = prediction.split(' ')\n",
    "        for idx in range(len(prediction)):\n",
    "            if 'true' in prediction[idx]:\n",
    "                keywords.append(candidates[idx])\n",
    "        result = ', '.join(keywords)\n",
    "        results.append(result)\n",
    "                \n",
    "    return results\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536/536 [1:04:22<00:00,  7.21s/it]\n",
      "100%|██████████| 536/536 [48:02<00:00,  5.38s/it]\n",
      "100%|██████████| 536/536 [1:08:19<00:00,  7.65s/it]\n"
     ]
    }
   ],
   "source": [
    "nb_rows = 34307\n",
    "#nb_rows = 128\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    prefix = model['prefix']\n",
    "    def extract_question(input_dict):\n",
    "        features = dict(input_dict)\n",
    "        id = features['id']\n",
    "        question = prefix + 'journal: jama title: ' + features[\"title\"] +  ' abstract: ' + tf.strings.lower(features[\"abstract\"])\n",
    "        return (id,question)\n",
    "\n",
    "    raw_train_data = read_session(TRAIN_TABLE_ID).parallel_read_rows().map(extract_question).batch(BATCH_SIZE)\n",
    "    model_name = model['model_name']\n",
    "    model_size = model['model_size']\n",
    "    model_version = model['model_version']\n",
    "    model_number = model['model_number']\n",
    "    saved_model_path = f'gs://antoine-vs-t5/{model_name}/{model_size}/{model_version}/export/{model_number}' #large\n",
    "\n",
    "    def load_predict_fn(model_path):\n",
    "        imported = tf.saved_model.load(saved_model_path, [\"serve\"])\n",
    "        return lambda x: imported.signatures['serving_default'](tf.constant(x))['outputs'].numpy()\n",
    "\n",
    "\n",
    "    predict_fn = load_predict_fn(saved_model_path)\n",
    "    \n",
    "    predictions = np.array([])\n",
    "    ids = np.array([])\n",
    "    for _ in tqdm(range(int(nb_rows / batch_size // 1))):\n",
    "        a = next(iter(raw_train_data))\n",
    "        ids_batch = a[0].numpy().astype('str')\n",
    "        ids = np.concatenate((ids, ids_batch), axis=0)\n",
    "        predictions_batch = generate_keywords(prediction(a[1].numpy()), model['candidates'])\n",
    "        predictions = np.concatenate((predictions, predictions_batch), axis=0)\n",
    "    df = pd.DataFrame([ids, predictions]).T\n",
    "    df.columns = ['ids','predictions']\n",
    "    df.to_csv(f'predictions-{TRAIN_TABLE_ID}-{model_name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
